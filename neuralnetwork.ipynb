{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaILe7hVBo0di/w17RuCGN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lk0099/expert-fortnight/blob/main/neuralnetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2ly0XW3KxMB",
        "outputId": "806e1607-72e2-4a10-da99-bbff4cc6fa43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.2703 - accuracy: 0.9230 - val_loss: 0.1370 - val_accuracy: 0.9611\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 6s 13ms/step - loss: 0.1029 - accuracy: 0.9687 - val_loss: 0.1033 - val_accuracy: 0.9697\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.0686 - accuracy: 0.9789 - val_loss: 0.0864 - val_accuracy: 0.9721\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0492 - accuracy: 0.9844 - val_loss: 0.0745 - val_accuracy: 0.9773\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0373 - accuracy: 0.9882 - val_loss: 0.0739 - val_accuracy: 0.9772\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0281 - accuracy: 0.9909 - val_loss: 0.0718 - val_accuracy: 0.9787\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0204 - accuracy: 0.9939 - val_loss: 0.0813 - val_accuracy: 0.9764\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0193 - accuracy: 0.9938 - val_loss: 0.0744 - val_accuracy: 0.9790\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 4s 9ms/step - loss: 0.0154 - accuracy: 0.9949 - val_loss: 0.0798 - val_accuracy: 0.9810\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0126 - accuracy: 0.9960 - val_loss: 0.0758 - val_accuracy: 0.9791\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.0758 - accuracy: 0.9791\n",
            "Test loss: 0.0758\n",
            "Test accuracy: 0.9791\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "x_train = x_train.reshape(-1, 28*28) / 255.0\n",
        "x_test = x_test.reshape(-1, 28*28) / 255.0\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Build the MLP model\n",
        "model = Sequential()\n",
        "model.add(Dense(256, activation='relu', input_shape=(28*28,)))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test loss: {loss:.4f}\")\n",
        "print(f\"Test accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load pre-trained model for object detection\n",
        "#net = cv2.dnn.readNetFromCaffe(\"deploy.prototxt\", \"model.caffemodel\")\n",
        "\n",
        "# Read video\n",
        "video_path = \"/content/WIN_20230708_16_42_33_Pro.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Define colors for bounding boxes\n",
        "colors = np.random.uniform(0, 255, size=(80, 3))\n",
        "\n",
        "# Process frames\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Perform object detection on the frame\n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 0.007843, (300, 300), 127.5)\n",
        "    net.setInput(blob)\n",
        "    detections = net.forward()\n",
        "\n",
        "    # Iterate over detected objects\n",
        "    for i in range(detections.shape[2]):\n",
        "        confidence = detections[0, 0, i, 2]\n",
        "        if confidence > 0.5:\n",
        "            class_id = int(detections[0, 0, i, 1])\n",
        "            class_name = classes[class_id]\n",
        "\n",
        "            # Get bounding box coordinates\n",
        "            box = detections[0, 0, i, 3:7] * np.array([width, height, width, height])\n",
        "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "            # Draw bounding box and label on the frame\n",
        "            color = colors[class_id]\n",
        "            cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
        "            label = f\"{class_name}: {confidence:.2f}\"\n",
        "            y = startY - 15 if startY - 15 > 15 else startY + 15\n",
        "            cv2.putText(frame, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "    # Display the resulting frame\n",
        "    cv2.imshow(\"Object Detection\", frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the video capture and close windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "8mYxaiGAMck2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Load pre-trained model for object detection\n",
        "#net = cv2.dnn.readNetFromCaffe(\"deploy.prototxt\", \"model.caffemodel\")\n",
        "\n",
        "# Initialize object tracker\n",
        "tracker = cv2.TrackerCSRT_create()\n",
        "\n",
        "# Read video\n",
        "video_path = \"input_video.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Define colors for bounding boxes\n",
        "colors = np.random.uniform(0, 255, size=(80, 3))\n",
        "\n",
        "# Initialize variables\n",
        "bbox = None\n",
        "tracking_started = False\n",
        "\n",
        "# Process frames\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    if not tracking_started:\n",
        "        # Perform object detection on the first frame\n",
        "        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 0.007843, (300, 300), 127.5)\n",
        "        net.setInput(blob)\n",
        "        detections = net.forward()\n",
        "\n",
        "        # Iterate over detected objects and select one for tracking\n",
        "        for i in range(detections.shape[2]):\n",
        "            confidence = detections[0, 0, i, 2]\n",
        "            if confidence > 0.5:\n",
        "                box = detections[0, 0, i, 3:7] * np.array([width, height, width, height])\n",
        "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "                bbox = (startX, startY, endX - startX, endY - startY)\n",
        "                tracker.init(frame, bbox)\n",
        "                tracking_started = True\n",
        "                break\n",
        "\n",
        "    else:\n",
        "        # Update tracker with the new frame\n",
        "        success, bbox = tracker.update(frame)\n",
        "\n",
        "        # Draw bounding box on the frame\n",
        "        if success:\n",
        "            (x, y, w, h) = [int(v) for v in bbox]\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "    # Display the resulting frame\n",
        "    cv2.imshow(\"Object Tracking\", frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the video capture and close windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "n8Kp4XoyMdTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Load pre-trained model for object detection\n",
        "net = cv2.dnn.readNetFromCaffe(\"deploy.prototxt\", \"model.caffemodel\")\n",
        "\n",
        "# Initialize object tracker\n",
        "tracker = cv2.TrackerCSRT_create()\n",
        "\n",
        "# Read video\n",
        "video_path = \"/content/WIN_20230708_16_42_33_Pro.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get video properties\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "# Define codec and create VideoWriter object\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "output_video = cv2.VideoWriter(\"output_video.avi\", fourcc, fps, (width, height))\n",
        "\n",
        "# Initialize variables\n",
        "bbox = None\n",
        "tracking_started = False\n",
        "\n",
        "# Process frames\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    if not tracking_started:\n",
        "        # Perform object detection on the first frame\n",
        "        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 0.007843, (300, 300), 127.5)\n",
        "        net.setInput(blob)\n",
        "        detections = net.forward()\n",
        "\n",
        "        # Iterate over detected objects and select one for tracking\n",
        "        for i in range(detections.shape[2]):\n",
        "            confidence = detections[0, 0, i, 2]\n",
        "            if confidence > 0.5:\n",
        "                box = detections[0, 0, i, 3:7] * np.array([width, height, width, height])\n",
        "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "                bbox = (startX, startY, endX - startX, endY - startY)\n",
        "                tracker.init(frame, bbox)\n",
        "                tracking_started = True\n",
        "                break\n",
        "\n",
        "    else:\n",
        "        # Update tracker with the new frame\n",
        "        success, bbox = tracker.update(frame)\n",
        "\n",
        "        # Draw bounding box on the frame\n",
        "        if success:\n",
        "            (x, y, w, h) = [int(v) for v in bbox]\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "    # Write frame with bounding box to output video\n",
        "    output_video.write(frame)\n",
        "\n",
        "    # Display the resulting frame\n",
        "    cv2.imshow(\"Object Tracking\", frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the video capture and close windows\n",
        "cap.release()\n",
        "output_video.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "HdXzp9Z4OjXr",
        "outputId": "68ecc294-d5d2-4f34-c9d9-db2af9ba1282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-98f629faaf1e>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load pre-trained model for object detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadNetFromCaffe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://figshare.com/articles/thesis/deploy_prototxt/8206211/1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model.caffemodel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Initialize object tracker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) /io/opencv/modules/dnn/src/caffe/caffe_io.cpp:1126: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"https://figshare.com/articles/thesis/deploy_prototxt/8206211/1\" in function 'ReadProtoFromTextFile'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U8mGY9ekPMdd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}